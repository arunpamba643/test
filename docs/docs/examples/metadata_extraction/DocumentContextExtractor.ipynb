{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Retrieval With Llama Index\n",
    "\n",
    "This notebook covers contextual retrieval with llama_index DocumentContextExtractor\n",
    "\n",
    "Based on an Anthropic [blost post](https://www.anthropic.com/news/contextual-retrieval), the concept is to:\n",
    "1. Use an LLM to generate a 'context' for each chunk based on the entire document\n",
    "2. embed the chunk + context together\n",
    "3. reap the benefits of higher RAG accuracy\n",
    "\n",
    "While you can also do this manually, the DocumentContextExtractor offers a lot of convenience and error handling, plus you can integrate it into your llama_index pipelines! Let's get started.\n",
    "\n",
    "NOTE: This notebook costs about $0.02 everytime you run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index\n",
    "%pip install llama-index-readers-file\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup an LLM\n",
    "You can use the MockLLM or you can use a real LLM of your choice here. flash 2 and gpt-4o-mini work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "OPENAI_API_KEY = \"\"\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Setup a data pipeline\n",
    "\n",
    " we'll need an embedding model, an index store, a vectore store, and a way to split tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pipeline & Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core.storage.docstore.simple_docstore import (\n",
    "    SimpleDocumentStore,\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Initialize document store and embedding model\n",
    "docstore = SimpleDocumentStore()\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create storage contexts\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "storage_context_no_extra_context = StorageContext.from_defaults(\n",
    "    docstore=docstore\n",
    ")\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=128, chunk_overlap=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DocumentContextExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the new part!\n",
    "\n",
    "from llama_index.core.extractors import DocumentContextExtractor\n",
    "\n",
    "context_extractor = DocumentContextExtractor(\n",
    "    # mandatory\n",
    "    docstore=docstore,\n",
    "    max_context_length=128000,\n",
    "    # optional\n",
    "    llm=llm,  # default to Settings.llm\n",
    "    oversized_document_strategy=\"warn\",\n",
    "    max_output_tokens=100,\n",
    "    key=\"context\",\n",
    "    prompt=DocumentContextExtractor.SUCCINCT_CONTEXT_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=[],\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    transformations=[text_splitter, context_extractor],\n",
    ")\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "index_nocontext = VectorStoreIndex.from_documents(\n",
    "    documents=[],\n",
    "    storage_context=storage_context_no_extra_context,\n",
    "    embed_model=embed_model,\n",
    "    transformations=[text_splitter],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/_ambiguated.txt\" \"paul_graham_essay_ambiguated.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"paul_graham_essay_ambiguated.txt\"]\n",
    ")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the pipeline, then search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:12<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# have to keep this updated for the DocumentContextExtractor to function.\n",
    "# everytime we insert a doc the entire pipeline will run and context will be generated\n",
    "storage_context.docstore.add_documents(documents)\n",
    "for doc in documents:\n",
    "    index.insert(doc)\n",
    "\n",
    "storage_context_no_extra_context.docstore.add_documents(documents)\n",
    "for doc in documents:\n",
    "    index_nocontext.insert(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all nodes have context\n",
    "assert context_extractor.is_job_complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "NO CONTEXT\n",
      "\n",
      "Chunk 1:\n",
      "Score: 0.47188236572598097\n",
      "Content: McCarthy's interpreter into IBM 704 machine language, and from then on Lisp also became a programming language in the conventional sense. But its origins as a model of computation gave it a power and elegance that other languages couldn't match. This quality was what attracted me in college, though I didn't understand why at the time.\n",
      "McCarthy's 1960 version did nothing more than interpret Lisp expressions. It was missing many features you'd want in a programming language. So these had to be added, and when they were, they weren't defined using his original\n",
      "\n",
      "Chunk 2:\n",
      "Score: 0.3765542523138754\n",
      "Content: is a general lesson here that our experience with Y Combinator also teaches: Customs continue to constrain you long after the restrictions that caused them have disappeared. Customary VC practice had once, like the customs about publishing essays, been based on real constraints. Startups had once been much more expensive to start, and proportionally rare. Now they could be cheap and common, but the VCs' customs still reflected the old world, just as customs about writing essays still reflected the constraints of the print era.\n",
      "Which in turn implies that people who are independent-minded\n",
      "\n",
      "Chunk 3:\n",
      "Score: 0.35159189802567903\n",
      "Content: painting, though the long sitting does tend to produce pained expressions in the sitters.\n",
      "[5] Interleaf was one of many companies that had smart people and built impressive technology, and yet got crushed by Moore's Law. In the 1990s the exponential growth in the power of commodity (i.e. Intel) processors rolled up high-end, special-purpose hardware and software companies like a bulldozer.\n",
      "[6] The signature style seekers at RISD weren't specifically mercenary. In the art world, money and coolness are tightly\n",
      "\n",
      "Chunk 4:\n",
      "Score: 0.3480829115663658\n",
      "Content: so I was 13 or 14. The district's machine happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. The space was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights. The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the reader and press a button\n",
      "\n",
      "Chunk 5:\n",
      "Score: 0.3455050264065611\n",
      "Content: microcomputers, everything changed. Now you could have one sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punched inputs and then stopping.\n",
      "\n",
      "I shifted to writing essays again, and created several new ones over the next few months. Some even ventured beyond startup topics. Then in March 2015 I began working on Lisp again.\n",
      "Lisp's unique characteristic is that its core is a language defined by writing an interpreter in itself. It wasn't originally intended\n",
      "==========\n",
      "WITH CONTEXT\n",
      "\n",
      "Chunk 1:\n",
      "Score: 0.48924416243430086\n",
      "Content: then stack them in the reader and press a button to load the code into memory and run it. The result would ordinarily be to print something on the spectacularly loud device.  I was puzzled by the machine. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on cards, and I didn't have any information stored on them. The only other option was to do things that didn't rely on any input, like calculate approximations\n",
      "\n",
      "Chunk 2:\n",
      "Score: 0.4593545360842972\n",
      "Content: so I was 13 or 14. The district's machine happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. The space was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights. The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the reader and press a button\n",
      "\n",
      "Chunk 3:\n",
      "Score: 0.3954230342481971\n",
      "Content: painting, though the long sitting does tend to produce pained expressions in the sitters.\n",
      "[5] Interleaf was one of many companies that had smart people and built impressive technology, and yet got crushed by Moore's Law. In the 1990s the exponential growth in the power of commodity (i.e. Intel) processors rolled up high-end, special-purpose hardware and software companies like a bulldozer.\n",
      "[6] The signature style seekers at RISD weren't specifically mercenary. In the art world, money and coolness are tightly\n",
      "\n",
      "Chunk 4:\n",
      "Score: 0.3928039211803362\n",
      "Content: microcomputers, everything changed. Now you could have one sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punched inputs and then stopping.\n",
      "\n",
      "I shifted to writing essays again, and created several new ones over the next few months. Some even ventured beyond startup topics. Then in March 2015 I began working on Lisp again.\n",
      "Lisp's unique characteristic is that its core is a language defined by writing an interpreter in itself. It wasn't originally intended\n",
      "\n",
      "Chunk 5:\n",
      "Score: 0.3728835052221352\n",
      "Content: Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. They were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep. The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The\n"
     ]
    }
   ],
   "source": [
    "test_question = \"Which chunks of text discuss the IBM 704?\"\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "nodes_fromcontext = retriever.retrieve(test_question)\n",
    "\n",
    "retriever_nocontext = index_nocontext.as_retriever(similarity_top_k=5)\n",
    "nodes_nocontext = retriever_nocontext.retrieve(test_question)\n",
    "# Print each node's content\n",
    "print(\"==========\")\n",
    "print(\"NO CONTEXT\")\n",
    "for i, node in enumerate(nodes_nocontext, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"Score: {node.score}\")  # Similarity score\n",
    "    print(f\"Content: {node.node.text}\")  # The actual text content\n",
    "\n",
    "# Print each node's content\n",
    "print(\"==========\")\n",
    "print(\"WITH CONTEXT\")\n",
    "for i, node in enumerate(nodes_fromcontext, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"Score: {node.score}\")  # Similarity score\n",
    "    print(f\"Content: {node.node.text}\")  # The actual text content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The no-context retriever does well at getting the exact match (IBM 704), but several chunks (2,3) seem totally irrelevant. The context retriever manages to get multiple chunks discussing Graham programming on the IBM 704. Unfortunately it totally misses the chunk that directly references the 704 directly by name, showing somewhat of a tradeoff, and the need for hybrid search - which Anthropic does mention in their blog post. You may have different results with different embedding models, prompts, and contextualization LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the index and vectorstore, cause it can take time and money to generate context!\n",
    "\n",
    "# for google drive support\n",
    "# persist_dir = '/content/drive/MyDrive/your_project_folder'\n",
    "persist_dir = \"./\"\n",
    "storage_context.persist(persist_dir=persist_dir)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
