{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Retrieval With Llama Index\n",
    "\n",
    "This notebook covers contextual retrieval with llama_index DocumentContextExtractor\n",
    "\n",
    "Based on an Anthropic [blost post](https://www.anthropic.com/news/contextual-retrieval), the concept is to:\n",
    "1. Use an LLM to generate a 'context' for each chunk based on the entire document\n",
    "2. embed the chunk + context together\n",
    "3. reap the benefits of higher RAG accuracy\n",
    "\n",
    "While you can also do this manually, the DocumentContextExtractor offers a lot of convenience and error handling, plus you can integrate it into your llama_index pipelines! Let's get started.\n",
    "\n",
    "NOTE: This notebook costs about $0.02 everytime you run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/users/cklap/llama_index/llama-index-core\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.12.9) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (3.11.10)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (1.0.8)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (0.2.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (2024.10.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (2.0.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (11.0.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (2.10.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from llama-index-core==0.12.9) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.12.9) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.12.9) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.12.9) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.12.9) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.12.9) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.12.9) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.12.9) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.12.9) (1.18.3)\n",
      "Requirement already satisfied: click in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core==0.12.9) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core==0.12.9) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core==0.12.9) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core==0.12.9) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core==0.12.9) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core==0.12.9) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core==0.12.9) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core==0.12.9) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core==0.12.9) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.12.9) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core==0.12.9) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core==0.12.9) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core==0.12.9) (3.23.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from httpx->llama-index-core==0.12.9) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from httpx->llama-index-core==0.12.9) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core==0.12.9) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.12.9) (24.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from anyio->httpx->llama-index-core==0.12.9) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\cklap\\llama_index\\.venv\\lib\\site-packages (from anyio->httpx->llama-index-core==0.12.9) (1.3.1)\n",
      "Building wheels for collected packages: llama-index-core\n",
      "  Building editable for llama-index-core (pyproject.toml): started\n",
      "  Building editable for llama-index-core (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-index-core: filename=llama_index_core-0.12.9-py3-none-any.whl size=2861 sha256=4cfeb1883041b485e67212c7f5068451dc09f3ad6eb609adc9dd4ede00495cdf\n",
      "  Stored in directory: C:\\Users\\cklap\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ntmuxh7o\\wheels\\39\\be\\cc\\d79d89f0d6850bfe4442c4da9e65b99968d47479d6dc183a45\n",
      "Successfully built llama-index-core\n",
      "Installing collected packages: llama-index-core\n",
      "  Attempting uninstall: llama-index-core\n",
      "    Found existing installation: llama-index-core 0.12.9\n",
      "    Uninstalling llama-index-core-0.12.9:\n",
      "      Successfully uninstalled llama-index-core-0.12.9\n",
      "Successfully installed llama-index-core-0.12.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index\n",
    "%pip install llama-index-readers-file\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup an LLM\n",
    "You can use the MockLLM or you can use a real LLM of your choice here. flash 2 and gpt-4o-mini work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "OPENAI_API_KEY = \"\"\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Setup a data pipeline\n",
    "\n",
    " we'll need an embedding model, an index store, a vectore store, and a way to split tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pipeline & Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core.storage.docstore.simple_docstore import (\n",
    "    SimpleDocumentStore,\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Initialize document store and embedding model\n",
    "docstore = SimpleDocumentStore()\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create storage contexts\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "storage_context_no_extra_context = StorageContext.from_defaults(\n",
    "    docstore=docstore\n",
    ")\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=128, chunk_overlap=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DocumentContextExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the new part!\n",
    "\n",
    "from llama_index.core.extractors import DocumentContextExtractor\n",
    "\n",
    "context_extractor = DocumentContextExtractor(\n",
    "    # these 2 are mandatory\n",
    "    docstore=docstore,\n",
    "    max_context_length=128000,\n",
    "    # below are optional\n",
    "    llm=llm,  # default to Settings.llm\n",
    "    oversized_document_strategy=\"warn\",\n",
    "    max_output_tokens=100,\n",
    "    key=\"context\",\n",
    "    prompt=DocumentContextExtractor.SUCCINCT_CONTEXT_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=[],\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    transformations=[text_splitter, context_extractor],\n",
    ")\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "index_nocontext = VectorStoreIndex.from_documents(\n",
    "    documents=[],\n",
    "    storage_context=storage_context_no_extra_context,\n",
    "    embed_model=embed_model,\n",
    "    transformations=[text_splitter],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay_ambiguated.txt\" -O \"paul_graham_essay_ambiguated.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"paul_graham_essay_ambiguated.txt\"]\n",
    ")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the pipeline, then search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "WARNING:root:'text' is deprecated and 'text_resource' will be used instead\n",
      "100%|██████████| 32/32 [00:10<00:00,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# have to keep this updated for the DocumentContextExtractor to function.\n",
    "# everytime we insert a doc the entire pipeline will run and context will be generated\n",
    "storage_context.docstore.add_documents(documents)\n",
    "for doc in documents:\n",
    "    index.insert(doc)\n",
    "\n",
    "storage_context_no_extra_context.docstore.add_documents(documents)\n",
    "for doc in documents:\n",
    "    index_nocontext.insert(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = \"Which chunks of text discuss the IBM 704?\"\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "nodes_fromcontext = retriever.retrieve(test_question)\n",
    "\n",
    "retriever_nocontext = index_nocontext.as_retriever(similarity_top_k=5)\n",
    "nodes_nocontext = retriever_nocontext.retrieve(test_question)\n",
    "\n",
    "# Verify all jobs were sucessfully completed\n",
    "assert all(node.metadata.get(\"context\") for node in nodes_fromcontext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "NO CONTEXT\n",
      "\n",
      "Chunk 1:\n",
      "Score: 0.47188236572598097\n",
      "Content: McCarthy's interpreter into IBM 704 machine language, and from then on Lisp also became a programming language in the conventional sense. But its origins as a model of computation gave it a power and elegance that other languages couldn't match. This quality was what attracted me in college, though I didn't understand why at the time.\n",
      "McCarthy's 1960 version did nothing more than interpret Lisp expressions. It was missing many features you'd want in a programming language. So these had to be added, and when they were, they weren't defined using his original\n",
      "\n",
      "Chunk 2:\n",
      "Score: 0.3765542523138754\n",
      "Content: is a general lesson here that our experience with Y Combinator also teaches: Customs continue to constrain you long after the restrictions that caused them have disappeared. Customary VC practice had once, like the customs about publishing essays, been based on real constraints. Startups had once been much more expensive to start, and proportionally rare. Now they could be cheap and common, but the VCs' customs still reflected the old world, just as customs about writing essays still reflected the constraints of the print era.\n",
      "Which in turn implies that people who are independent-minded\n",
      "\n",
      "Chunk 3:\n",
      "Score: 0.3664437165399054\n",
      "Content: was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives. So I wrote a more detailed version for others to read, and this is the last sentence of it.\n",
      "\n",
      "[1] My experience skipped a step in the evolution of computers: time-sharing machines with interactive OSes. I went straight from batch processing to microcomputers, which made the latter seem all the more exciting.\n",
      "[2] Italian\n",
      "\n",
      "Chunk 4:\n",
      "Score: 0.35159189802567903\n",
      "Content: painting, though the long sitting does tend to produce pained expressions in the sitters.\n",
      "[5] Interleaf was one of many companies that had smart people and built impressive technology, and yet got crushed by Moore's Law. In the 1990s the exponential growth in the power of commodity (i.e. Intel) processors rolled up high-end, special-purpose hardware and software companies like a bulldozer.\n",
      "[6] The signature style seekers at RISD weren't specifically mercenary. In the art world, money and coolness are tightly\n",
      "\n",
      "Chunk 5:\n",
      "Score: 0.3480829115663658\n",
      "Content: so I was 13 or 14. The district's machine happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. The space was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights. The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the reader and press a button\n",
      "==========\n",
      "WITH CONTEXT\n",
      "\n",
      "Chunk 1:\n",
      "Score: 0.46357148181765917\n",
      "Content: then stack them in the reader and press a button to load the code into memory and run it. The result would ordinarily be to print something on the spectacularly loud device.  I was puzzled by the machine. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on cards, and I didn't have any information stored on them. The only other option was to do things that didn't rely on any input, like calculate approximations\n",
      "\n",
      "Chunk 2:\n",
      "Score: 0.4522842638008198\n",
      "Content: so I was 13 or 14. The district's machine happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. The space was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights. The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the reader and press a button\n",
      "\n",
      "Chunk 3:\n",
      "Score: 0.3872088756342579\n",
      "Content: microcomputers, everything changed. Now you could have one sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punched inputs and then stopping.\n",
      "\n",
      "I shifted to writing essays again, and created several new ones over the next few months. Some even ventured beyond startup topics. Then in March 2015 I began working on Lisp again.\n",
      "Lisp's unique characteristic is that its core is a language defined by writing an interpreter in itself. It wasn't originally intended\n",
      "\n",
      "Chunk 4:\n",
      "Score: 0.38427663325151606\n",
      "Content: painting, though the long sitting does tend to produce pained expressions in the sitters.\n",
      "[5] Interleaf was one of many companies that had smart people and built impressive technology, and yet got crushed by Moore's Law. In the 1990s the exponential growth in the power of commodity (i.e. Intel) processors rolled up high-end, special-purpose hardware and software companies like a bulldozer.\n",
      "[6] The signature style seekers at RISD weren't specifically mercenary. In the art world, money and coolness are tightly\n",
      "\n",
      "Chunk 5:\n",
      "Score: 0.3795984684266392\n",
      "Content: Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. They were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep. The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The\n"
     ]
    }
   ],
   "source": [
    "# Print each node's content\n",
    "print(\"==========\")\n",
    "print(\"NO CONTEXT\")\n",
    "for i, node in enumerate(nodes_nocontext, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"Score: {node.score}\")  # Similarity score\n",
    "    print(f\"Content: {node.node.text}\")  # The actual text content\n",
    "\n",
    "# Print each node's content\n",
    "print(\"==========\")\n",
    "print(\"WITH CONTEXT\")\n",
    "for i, node in enumerate(nodes_fromcontext, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"Score: {node.score}\")  # Similarity score\n",
    "    print(f\"Content: {node.node.text}\")  # The actual text content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The no-context retriever does well at getting the exact match (IBM 704), but several chunks (2,3) seem totally irrelevant. The context retriever manages to get multiple chunks discussing Graham programming on the IBM 704. Unfortunately it totally misses the chunk that directly references the 704 directly by name, showing somewhat of a tradeoff, and the need for hybrid search - which Anthropic does mention in their blog post. You may have different results with different embedding models, prompts, and contextualization LLMs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
