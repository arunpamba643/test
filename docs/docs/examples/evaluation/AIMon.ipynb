{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1r5MH4aIzxT4cNOrEetH0ZvyTTY0UQRNP#scrollTo=QAU2NLcx3_iJ)"
      ],
      "metadata": {
        "id": "QAU2NLcx3_iJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zQPfkLWMfV5"
      },
      "source": [
        "## AIMon's LlamaIndex Extension for LLM Response Evaluation\n",
        "\n",
        "This notebook introduces AIMon's evaluators for the LlamaIndex framework, which are designed to assess the quality and accuracy of responses generated by language models (LLMs) integrated into LlamaIndex. Below is an overview of all available evaluators:\n",
        "\n",
        "- **Hallucination Evaluator**: Detects when a model generates information not supported by the provided context (hallucinations).\n",
        "- **Guideline Evaluator**: Ensures model responses follow predefined instructions and guidelines.\n",
        "- **Completeness Evaluator**: Checks whether the response fully addresses all aspects of the query or task.\n",
        "- **Conciseness Evaluator**: Evaluates if the response is brief yet complete, avoiding unnecessary verbosity.\n",
        "- **Toxicity Evaluator**: Flags harmful, offensive, or inappropriate language in the response.\n",
        "- **Context Relevance Evaluator**: Assesses the relevance and accuracy of the provided context in supporting the model's response.\n",
        "\n",
        "In this notebook, we will focus on utilizing the **Hallucination Evaluator**, **Guideline Evaluator**, and **Context Relevance Evaluator** to improve your RAG (Retrieval-Augmented Generation) applications.\n",
        "\n",
        "To learn more about AIMon, check out these resources:: [Website](https://www.aimon.ai/) and [Documentation](https://docs.aimon.ai/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sm9HtZOLWdr"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BziCIBWkOHlN"
      },
      "source": [
        "Let's get started by installing the dependencies and setting up the API keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hRWiH4gbdu18"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install requests datasets aimon-llamaindex llama-index-embeddings-openai llama-index-llms-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure your `OPENAI_API_KEY` and `AIMON_API_KEY` in Google Collab secrets and provide them notebook access. We will use OpenAI for the LLM and embedding generation models. We will use AIMon for continuous monitoring of quality issues.\n",
        "\n",
        "AIMon API key can be obtained [here](https://docs.aimon.ai/quickstart#1-api-key)."
      ],
      "metadata": {
        "id": "H8o3k9cH4nS1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tDGfYXdD_p1L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "# Import Colab Secrets userdata module.\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f7A2lGMMylS"
      },
      "source": [
        "## Dataset for evaluation\n",
        "\n",
        "In this example, we will be using the transcripts from MeetingBank dataset [1] as our contextual information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cVl26fHjBsVW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from datasets import load_dataset\n",
        "meetingbank = load_dataset(\"huuuyeah/meetingbank\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztPF3iwcSasy"
      },
      "source": [
        "This function helps extract transcripts and converts them into a list of objects of type `llama_index.core.Document`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w7VdAPooSXA5"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "def extract_and_create_documents(transcripts):\n",
        "\n",
        "    documents = []\n",
        "\n",
        "    for transcript in transcripts:\n",
        "\n",
        "        try:\n",
        "\n",
        "          doc = Document(text=transcript)\n",
        "          documents.append(doc)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create document\")\n",
        "\n",
        "    return documents\n",
        "\n",
        "transcripts = [meeting['transcript'] for meeting in meetingbank['train']]\n",
        "documents = extract_and_create_documents(transcripts[:5])  ## Using only 5 transcripts to keep this example fast and concise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFykVHKbM6B6"
      },
      "source": [
        "Set up an embedding model. We will be using the `text-embedding-3-small` model here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y1ChtzenERv4"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "embedding_model = OpenAIEmbedding(model=\"text-embedding-3-small\", embed_batch_size=100, max_retries = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Gp7A7WCHQQ"
      },
      "source": [
        "Split documents into nodes and generate their embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "klF-m9AvC70z"
      },
      "outputs": [],
      "source": [
        "from aimon_llamaindex import generate_embeddings_for_docs\n",
        "nodes = generate_embeddings_for_docs(documents, embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYvg6IAeE0eY"
      },
      "source": [
        "Insert the nodes with embeddings into in-memory Vector Store Index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X_4270PnFs_-"
      },
      "outputs": [],
      "source": [
        "from aimon_llamaindex import build_index\n",
        "\n",
        "index = build_index(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1qfHCAH1If6"
      },
      "source": [
        "Instantiate a Vector Index Retrieiver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "52zamq1_1Hhd"
      },
      "outputs": [],
      "source": [
        "from aimon_llamaindex import build_retriever\n",
        "\n",
        "retriever = build_retriever(index, similarity_top_k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-25hYRKUVfw"
      },
      "source": [
        "## Building the LLM Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzxo_kxTI0VW"
      },
      "source": [
        "Configure the Large Language Model. Here we choose OpenAI's `gpt-4o-mini` model with temperature setting of 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nTu-W1h_9GQ0"
      },
      "outputs": [],
      "source": [
        "## OpenAI's LLM\n",
        "from llama_index.llms.openai import OpenAI\n",
        "llm = OpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.4,\n",
        "    system_prompt =  \"\"\"\n",
        "                    Please be professional and polite.\n",
        "                    Answer the user's question in a single line.\n",
        "                    Even if the context lacks information to answer the question, make\n",
        "                    sure that you answer the user's question based on your own knowledge.\n",
        "                    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-poWEqvNdib"
      },
      "source": [
        "Define your query and instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5rwiuVweerVx"
      },
      "outputs": [],
      "source": [
        "user_query = \"Which council bills were amended for zoning regulations?\"\n",
        "user_instructions = \"1. Keep the response concise, preferably under the 100 word limit.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr92vz2baco1"
      },
      "source": [
        "Update the LLM's system prompt with the user's instructions defined dynamically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "RpDQnvRpacWX"
      },
      "outputs": [],
      "source": [
        "llm.system_prompt += f\"Please comply to the following instructions {user_instructions}.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbDqYTPCmfov"
      },
      "source": [
        "Retrieve a response for the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "DweqLmHRmQLw"
      },
      "outputs": [],
      "source": [
        "from aimon_llamaindex import get_response\n",
        "llm_response = get_response(user_query, retriever, llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXKq8bKzU6KB"
      },
      "source": [
        "## Running Evaluations using AIMon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i_gUiMEkPC5"
      },
      "source": [
        "Configure AIMon Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "N297vvY1kN_v"
      },
      "outputs": [],
      "source": [
        "from aimon import Client\n",
        "aimon_client = Client(auth_header=\"Bearer {}\".format(userdata.get('AIMON_API_KEY')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrsTYj5CGu6h"
      },
      "source": [
        "Using AIMon’s Instruction Adherence Model (a.k.a. Guideline Evaluator)\n",
        "\n",
        "This model evaluates if generated text adheres to given instructions, ensuring that LLMs follow the user’s guidelines and intent across various tasks for more accurate and relevant outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "NOhO65AxqPaX"
      },
      "outputs": [],
      "source": [
        "from aimon_llamaindex.evaluators import GuidelineEvaluator\n",
        "\n",
        "guideline_evaluator = GuidelineEvaluator(aimon_client)\n",
        "evaluation_result = guideline_evaluator.evaluate(user_query, user_instructions, llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezCBV5L-qPaY",
        "outputId": "5a08a584-5880-4acf-c710-bd67b328eb0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"results\": [\n",
            "        {\n",
            "            \"adherence\": true,\n",
            "            \"detailed_explanation\": \"The response contains 60 words, which is well under the 100 word limit. It effectively summarizes the amendments and their intentions in a brief manner.\",\n",
            "            \"instruction\": \"Keep the response concise, preferably under the 100 word limit.\"\n",
            "        }\n",
            "    ],\n",
            "    \"score\": 1.0\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "## Printing the initial guideline adherence result\n",
        "print(json.dumps(evaluation_result, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "VlPJwwTQSkbh"
      },
      "outputs": [],
      "source": [
        "## Running a loop to improve guideline adherence\n",
        "current_attempt = 1\n",
        "max_attempts = 2\n",
        "reattemped_g = False\n",
        "\n",
        "while(evaluation_result['results'][0]['adherence']!=True):\n",
        "  if current_attempt > max_attempts:\n",
        "    break\n",
        "\n",
        "  reattemped_g = True\n",
        "  llm.system_prompt = f\"\"\"The last LLM response failed to comply with the instructions.\\\n",
        "                          Please adhere to the following instructions while generating the next response: {user_instructions}\"\"\"\n",
        "  llm_response = get_response(user_query, retriever, llm)\n",
        "  evalution_result = guideline_evaluator.evaluate(user_query, user_instructions, llm_response)\n",
        "  current_attempt+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVKC2htOU9QD",
        "outputId": "346e1c64-02c3-4bb4-c588-65b996c04d0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instructions were complied with in the first prompt to the LLM.\n"
          ]
        }
      ],
      "source": [
        "## Printing the final guideline adherence result, if the LLM was prompted again\n",
        "if reattemped_g == True:\n",
        "  print(json.dumps(evaluation_result, indent=4))\n",
        "else:\n",
        "  print(\"Instructions were complied with in the first prompt to the LLM.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owUKX1s5Gi5q"
      },
      "source": [
        "Using AIMon’s Hallucination Detection Evaluator Model (HDM-1) to improve the quality of responses obtained by the LLM application.\n",
        "\n",
        "AIMon’s HDM-1 detects hallucinated content in LLM outputs. It provides a “hallucination score” (0.0–1.0) quantifying the likelihood of factual inaccuracies or fabricated information, ensuring more reliable and accurate responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "GnrQwhLfqO6C"
      },
      "outputs": [],
      "source": [
        "from aimon_llamaindex.evaluators import HallucinationEvaluator\n",
        "\n",
        "hallucination_evaluator = HallucinationEvaluator(aimon_client)\n",
        "evalution_result = hallucination_evaluator.evaluate(user_query, user_instructions, llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaO236MxQ1Uz",
        "outputId": "40f7036b-70ec-4931-9ed7-c09c98a78d11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"is_hallucinated\": \"False\",\n",
            "    \"score\": 0.22446,\n",
            "    \"sentences\": [\n",
            "        {\n",
            "            \"score\": 0.22446,\n",
            "            \"text\": \"The council bills amended for zoning regulations include the small lot moratorium and the text amendment related to off-street parking exemptions for preexisting small lots. These amendments aim to balance the interests of local neighborhoods, health institutions, and developers.\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "## Printing the initial evaluation result for Hallucination\n",
        "print(json.dumps(evalution_result, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "m0QL4SqVOGTa"
      },
      "outputs": [],
      "source": [
        "## Let hallucination threshold be at 0.65\n",
        "hallucination_threshold = 0.65\n",
        "\n",
        "## Let maximum attempts to reduce hallucination be 2\n",
        "max_attempts = 2\n",
        "current_attempt = 1\n",
        "reattemped_h = False\n",
        "\n",
        "while(evalution_result['score'] > 0.65):\n",
        "\n",
        "  if current_attempt > max_attempts:\n",
        "    break\n",
        "\n",
        "  reattemped_h = True\n",
        "  llm.system_prompt=f\"The latest LLM response obtained was {llm_response}\\\n",
        "                      Upon evaluation, it was found out that this LLM response hallucinated with a score of {evalution_result['score']}\\\n",
        "                      Please generate a new response, and take into consideration the following factors:\\\n",
        "                      a. If the hallucination score is between 0 and 0.5, no special action is required for improving accuracy.\\\n",
        "                      b. If the hallucination score is between 0.5 and 0.75, please focus on reducing any noticeable inaccuracies, while ensuring the response is as reliable as possible.\\\n",
        "                      c. If the hallucination score is between 0.75 and 1, take extra care to minimize significant inaccuracies and ensure that the response is mostly factual and reliable, avoiding any fictional or misleading content.\\\n",
        "                      Also, make sure to comply with the following instructions {user_instructions} while generating new responses.\\\n",
        "                      \"\n",
        "  llm_response = get_response(user_query, retriever, llm)\n",
        "  evalution_result = hallucination_evaluator.evaluate(user_query, user_instructions, llm_response)\n",
        "  current_attempt+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWKDfbfHqO6H",
        "outputId": "7d6b9db8-2217-4a6f-a072-60ff047d6b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLM response received in the first run was not hallucinated. Therefore, the LLM was not prompted again to reduce hallucination.\n"
          ]
        }
      ],
      "source": [
        "## Print the final evaluation result for Hallucination, if the LLM was prompted again\n",
        "if reattemped_h == True:\n",
        "  print(json.dumps(evalution_result, indent=4))\n",
        "else:\n",
        "  print(\"The LLM response received in the first run was not hallucinated. Therefore, the LLM was not prompted again to reduce hallucination.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_oAKqHzyzG6"
      },
      "source": [
        "Using AIMon's Context Relevance Evaluator to evaluate the relevance of context data used by the LLM to generate the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "OjuUqCmfyyXs"
      },
      "outputs": [],
      "source": [
        "from aimon_llamaindex.evaluators import ContextRelevanceEvaluator\n",
        "\n",
        "evaluator = ContextRelevanceEvaluator(aimon_client)\n",
        "task_definition = \"Find the relevance of the context data used to generate this response.\"\n",
        "evaluation_result = evaluator.evaluate(user_query, user_instructions, llm_response, task_definition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSgx-UXzy1Pg",
        "outputId": "ef52c3f0-c2d2-45ca-b00e-990ce68ee23a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"explanations\": [\n",
            "            \"Document 1 discusses a council bill related to zoning regulations, specifically mentioning a text amendment that aims to balance neighborhood interests with developer needs. However, it primarily focuses on parking issues and personal experiences rather than detailing specific zoning regulation amendments or the council bills directly related to them, which makes it less relevant to the query.\",\n",
            "            \"2. Document 2 mentions zoning and development issues, including the need for mass transit and affordability, but it does not provide specific information on which council bills were amended for zoning regulations. The discussion is more about general concerns regarding development and transportation rather than direct references to zoning amendments.\",\n",
            "            \"3. Document 3 touches on zoning laws and amendments but does not specify which council bills were amended for zoning regulations. While it discusses the context of zoning and housing, it lacks concrete details that directly answer the query about specific bills.\",\n",
            "            \"4. Document 4 discusses broader issues about affordable housing and transportation without directly addressing any specific council bills or amendments related to zoning regulations. The focus is on general priorities and funding rather than specific legislative changes, making it less relevant to the query.\",\n",
            "            \"5. Document 5 mentions support for a zoning code amendment regarding parking exemptions for small lots, which is somewhat related to zoning regulations. However, it does not provide specific details about the council bills amended for zoning regulations, thus failing to fully address the query.\"\n",
            "        ],\n",
            "        \"query\": \"Which council bills were amended for zoning regulations?\",\n",
            "        \"relevance_scores\": [\n",
            "            40.5,\n",
            "            40.25,\n",
            "            44.25,\n",
            "            38.5,\n",
            "            43.0\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "print(json.dumps(evaluation_result, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH42Iu2B6Ibc"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we built a simple RAG application using the LlamaIndex framework. After retrieving a response to a query, we assessed it with AIMon’s evaluators and used the evaluation results to refine the model’s performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DafO72wIPUFH"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYFTZ4nZW8go"
      },
      "source": [
        "[1]. Y. Hu, T. Ganter, H. Deilamsalehy, F. Dernoncourt, H. Foroosh, and F. Liu, \"MeetingBank: A Benchmark Dataset for Meeting Summarization,\" arXiv, May 2023. [Online]. Available: https://arxiv.org/abs/2305.17529. Accessed: Jan. 16, 2025."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}